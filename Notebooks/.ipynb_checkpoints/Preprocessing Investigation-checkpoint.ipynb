{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed Heavily from https://www.kaggle.com/nitin194/twitter-sentiment-analysis-word2vec-doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off let's access some of the data files for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['TweetID', 'Sentiment', 'Tweet']\n",
    "\n",
    "train = pd.read_csv('../Datasets/dataset/train/twitter-2016train-A.txt', delimiter='\\t', names=names)\n",
    "test = pd.read_csv('../Datasets/dataset/train/twitter-2016test-A.txt', delimiter='\\t', names=['TweetID', 'Sentiment', 'Tweet', 'tidy_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine what the content looks like for positive, negative, and neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['Sentiment']=='positive'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['Sentiment']=='negative'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['Sentiment']=='neutral'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train.Tweet.str.len(), label='train', bins=14, range=[0,140])\n",
    "plt.hist(test.Tweet.str.len(), label='test', bins=14, range=[0,140])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we want to do some data cleaning to get rid of unwanted stuff not relevant to sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove several things out of raw tweets\n",
    "1. Twitter handles due to privacy concerns\n",
    "2. Punctuation, numbers, and special characters\n",
    "3. Small words\n",
    "4. Normalize textual data so we don't have different bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removing Twitter Handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tidy_tweet'] = np.vectorize(remove_pattern)(train['Tweet'], \"@[\\w]*\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Removing Punctuations, Numbers, and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tidy_tweet = train.tidy_tweet.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removing Short Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tidy_tweet = train.tidy_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = train.tidy_tweet.apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "train['tidy_tweet'] = tokenized_tweet\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for convenience sake I'll make a single function that does all that for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "def tidy_tweet(dataset, min_len=3):\n",
    "    dataset['tidy_tweet'] = np.vectorize(remove_pattern)(dataset['Tweet'], \"@[\\w]*\")\n",
    "    dataset.tidy_tweet = dataset.tidy_tweet.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    dataset.tidy_tweet = dataset.tidy_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > min_len]))\n",
    "    tokenized_tweet = dataset.tidy_tweet.apply(lambda x: x.split())\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    tokenized_tweet.head()\n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    dataset['tidy_tweet'] = tokenized_tweet\n",
    "    return dataset\n",
    "\n",
    "test = tidy_tweet(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step Is We Need to Train Word2Vec on our Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_tweet = train['tidy_tweet'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size = 200, #desired number of features, 200 seems to be a common width, no idea why\n",
    "            window = 5, #context window size\n",
    "            min_count =2, #ignores all words with total freq lower than 2\n",
    "            sg = 1, #encoding for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, #for negative sampling\n",
    "            workers = 2, #no. of cores\n",
    "            seed = 34\n",
    ")\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples = len(train['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of our embedding space is 200, that means that each word is represented by a vector of 200 floating point numbers, isn't that convenient :).\n",
    "\n",
    "But in order to make this dataset directly applicable to our neural networks we will need to produce a feature vector for all of the tweets.\n",
    "\n",
    "Now since we are eliminating any words that are less than 3 chars in length, the max number of words we could have is 140/5, because each word has a space after it,  but we have to account for the fact that there will always be n-1 spaces so we'll add one and make the vector length 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_w2v['trump'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to define a function that we can map onto our Tweets, the only reason being we'll get an error if the model can't find the word embedding. My solution to this is just to return a 0 vector if the word doesn't exist in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(model, token, dim=200):\n",
    "    try:\n",
    "        return model[token]\n",
    "    except(KeyError):\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "apply_model(model_w2v, 'schmup') #Testing, yep, this word doesn't exist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for tweet_ind in range(len(tokenized_tweet)):\n",
    "    feature_map = []\n",
    "    for word in tokenized_tweet[tweet_ind]:\n",
    "        feature_map.append(apply_model(model_w2v, word))\n",
    "    feature_map = np.array(feature_map)\n",
    "    zero_pad = np.zeros([29,200])\n",
    "    zero_pad[:feature_map.shape[0], :feature_map.shape[1]] = feature_map\n",
    "    feature_map = zero_pad\n",
    "    inputs.append(feature_map)\n",
    "    \n",
    "inputs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i,v in enumerate(tokenized_tweet) if len(v) > 55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet[1878]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
