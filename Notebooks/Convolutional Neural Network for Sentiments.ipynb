{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "INPUT_DIMS = (55, 200, 1)\n",
    "REGION_SIZES = [2,3,4]\n",
    "FILTERS_PER_REGION = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "def tidy_tweet(dataset, min_len=3):\n",
    "    dataset['tidy_tweet'] = np.vectorize(remove_pattern)(dataset['Tweet'], \"@[\\w]*\")\n",
    "    dataset.tidy_tweet = dataset.tidy_tweet.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    dataset.tidy_tweet = dataset.tidy_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > min_len]))\n",
    "    tokenized_tweet = dataset.tidy_tweet.apply(lambda x: x.split())\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    tokenized_tweet.head()\n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    dataset['tidy_tweet'] = tokenized_tweet\n",
    "    return dataset\n",
    "\n",
    "def tokenized_tweet(dataframe):\n",
    "    return dataframe['tidy_tweet'].apply(lambda x: x.split())\n",
    "\n",
    "def train_vectorizer(tokenized_tweet): \n",
    "    model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size = 200, #desired number of features, 200 seems to be a common width, no idea why\n",
    "            window = 5, #context window size\n",
    "            min_count =2, #ignores all words with total freq lower than 2\n",
    "            sg = 1, #encoding for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, #for negative sampling\n",
    "            workers = 2, #no. of cores\n",
    "            seed = 34\n",
    "    )\n",
    "\n",
    "    model_w2v.train(tokenized_tweet, total_examples = len(train['tidy_tweet']), epochs=20)\n",
    "    return model_w2v\n",
    "\n",
    "def apply_model(model, token, dim=200):\n",
    "    try:\n",
    "        return model[token]\n",
    "    except(KeyError):\n",
    "        return np.zeros(dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_inputs(tokenized_tweet, model_w2v):\n",
    "    inputs = np.zeros((len(tokenized_tweet), 55, 200, 1))\n",
    "    for tweet_ind in range(len(tokenized_tweet)):\n",
    "        feature_map = inputs[tweet_ind]\n",
    "        for word_ind, word in enumerate(tokenized_tweet[tweet_ind]):\n",
    "            if(word_ind<55):\n",
    "                feature_map[word_ind] = apply_model(model_w2v, word).reshape(200,1)\n",
    "            else:\n",
    "                return tweet_ind\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "names = ['TweetID', 'Sentiment', 'Tweet']\n",
    "\n",
    "train = pd.read_csv('../Datasets/dataset/train/twitter-2016train-A.txt', delimiter='\\t', names=names)\n",
    "\n",
    "#TODO- for some reason certain Tweets don't separate like they are supposed to, this first happens in line 1064 where\n",
    "#there are like 12 Tweets all rolled into one\n",
    "train = tidy_tweet(train)[:1063]\n",
    "tokenized_tweet = tokenized_tweet(train)\n",
    "model_w2v = train_vectorizer(tokenized_tweet)\n",
    "\n",
    "x_train = vectorize_inputs(tokenized_tweet, model_w2v)\n",
    "\n",
    "y_train = []\n",
    "for sentiment in train['Sentiment']:\n",
    "    if sentiment == 'positive':\n",
    "        y_train.append(1)\n",
    "    elif sentiment == 'negative':\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(0.5)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - 10s 343ms/step - loss: 0.9447 - accuracy: 0.1872 - val_loss: 0.9675 - val_accuracy: 0.0654\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 10s 325ms/step - loss: 0.8459 - accuracy: 0.1946 - val_loss: 0.9125 - val_accuracy: 0.0654\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 11s 362ms/step - loss: 0.8077 - accuracy: 0.1946 - val_loss: 0.8814 - val_accuracy: 0.0654\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 13s 439ms/step - loss: 0.7732 - accuracy: 0.1946 - val_loss: 0.8418 - val_accuracy: 0.0654\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 14s 480ms/step - loss: 0.7448 - accuracy: 0.1956 - val_loss: 0.8012 - val_accuracy: 0.0654\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 12s 393ms/step - loss: 0.7202 - accuracy: 0.1956 - val_loss: 0.7743 - val_accuracy: 0.0654\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 10s 337ms/step - loss: 0.7018 - accuracy: 0.1987 - val_loss: 0.7506 - val_accuracy: 0.0654\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 11s 352ms/step - loss: 0.6898 - accuracy: 0.1998 - val_loss: 0.7326 - val_accuracy: 0.0748\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 10s 343ms/step - loss: 0.6840 - accuracy: 0.2082 - val_loss: 0.7272 - val_accuracy: 0.0748\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 10s 349ms/step - loss: 0.6802 - accuracy: 0.2092 - val_loss: 0.7192 - val_accuracy: 0.0748\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 11s 357ms/step - loss: 0.6781 - accuracy: 0.2050 - val_loss: 0.7114 - val_accuracy: 0.0748\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6757 - accuracy: 0.2374 - val_loss: 0.7123 - val_accuracy: 0.0748\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 11s 351ms/step - loss: 0.6730 - accuracy: 0.2469 - val_loss: 0.7224 - val_accuracy: 0.0748\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 13s 438ms/step - loss: 0.6712 - accuracy: 0.2218 - val_loss: 0.7179 - val_accuracy: 0.0748\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 11s 376ms/step - loss: 0.6689 - accuracy: 0.2395 - val_loss: 0.7141 - val_accuracy: 0.0748\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 10s 349ms/step - loss: 0.6672 - accuracy: 0.2573 - val_loss: 0.7177 - val_accuracy: 0.0748\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 12s 383ms/step - loss: 0.6657 - accuracy: 0.2395 - val_loss: 0.7172 - val_accuracy: 0.0748\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 11s 350ms/step - loss: 0.6642 - accuracy: 0.3274 - val_loss: 0.7250 - val_accuracy: 0.0748\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6637 - accuracy: 0.2845 - val_loss: 0.7078 - val_accuracy: 0.1028\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6638 - accuracy: 0.2699 - val_loss: 0.7045 - val_accuracy: 0.1215\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 11s 371ms/step - loss: 0.6588 - accuracy: 0.3954 - val_loss: 0.7235 - val_accuracy: 0.0748\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 10s 349ms/step - loss: 0.6582 - accuracy: 0.2751 - val_loss: 0.7082 - val_accuracy: 0.1121\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 11s 369ms/step - loss: 0.6545 - accuracy: 0.3515 - val_loss: 0.7314 - val_accuracy: 0.0748\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6544 - accuracy: 0.3368 - val_loss: 0.7200 - val_accuracy: 0.0935\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 10s 342ms/step - loss: 0.6515 - accuracy: 0.3452 - val_loss: 0.7264 - val_accuracy: 0.0935\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 10s 338ms/step - loss: 0.6506 - accuracy: 0.3776 - val_loss: 0.7301 - val_accuracy: 0.0935\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.6489 - accuracy: 0.3588 - val_loss: 0.7173 - val_accuracy: 0.1028\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 11s 352ms/step - loss: 0.6466 - accuracy: 0.3682 - val_loss: 0.7182 - val_accuracy: 0.1028\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6453 - accuracy: 0.3567 - val_loss: 0.7185 - val_accuracy: 0.1121\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.40 - 11s 351ms/step - loss: 0.6456 - accuracy: 0.4038 - val_loss: 0.7332 - val_accuracy: 0.1028\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 10s 338ms/step - loss: 0.6423 - accuracy: 0.3849 - val_loss: 0.7142 - val_accuracy: 0.1682\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 10s 336ms/step - loss: 0.6403 - accuracy: 0.3797 - val_loss: 0.7231 - val_accuracy: 0.1402\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.6393 - accuracy: 0.4163 - val_loss: 0.7335 - val_accuracy: 0.1028\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 10s 342ms/step - loss: 0.6382 - accuracy: 0.3902 - val_loss: 0.7132 - val_accuracy: 0.1963\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 10s 337ms/step - loss: 0.6358 - accuracy: 0.4048 - val_loss: 0.7169 - val_accuracy: 0.1869\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 10s 334ms/step - loss: 0.6338 - accuracy: 0.3964 - val_loss: 0.7126 - val_accuracy: 0.2150\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 10s 343ms/step - loss: 0.6316 - accuracy: 0.4404 - val_loss: 0.7320 - val_accuracy: 0.1682\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 10s 337ms/step - loss: 0.6312 - accuracy: 0.4027 - val_loss: 0.7246 - val_accuracy: 0.1869\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 10s 346ms/step - loss: 0.6288 - accuracy: 0.4142 - val_loss: 0.7093 - val_accuracy: 0.2523\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 0.6260 - accuracy: 0.4236 - val_loss: 0.7239 - val_accuracy: 0.2056\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.6243 - accuracy: 0.4236 - val_loss: 0.7229 - val_accuracy: 0.2150\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 10s 332ms/step - loss: 0.6210 - accuracy: 0.4372 - val_loss: 0.7339 - val_accuracy: 0.1776\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 10s 334ms/step - loss: 0.6195 - accuracy: 0.4351 - val_loss: 0.7376 - val_accuracy: 0.1776\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 10s 332ms/step - loss: 0.6164 - accuracy: 0.4184 - val_loss: 0.7178 - val_accuracy: 0.2523\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 0.6139 - accuracy: 0.4435 - val_loss: 0.7379 - val_accuracy: 0.2056\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 11s 368ms/step - loss: 0.6113 - accuracy: 0.4351 - val_loss: 0.7345 - val_accuracy: 0.2056\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.6080 - accuracy: 0.4236 - val_loss: 0.7161 - val_accuracy: 0.2804\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 11s 352ms/step - loss: 0.6063 - accuracy: 0.4592 - val_loss: 0.7430 - val_accuracy: 0.1963\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 11s 361ms/step - loss: 0.6049 - accuracy: 0.4508 - val_loss: 0.7318 - val_accuracy: 0.2430\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 10s 334ms/step - loss: 0.6038 - accuracy: 0.4205 - val_loss: 0.7153 - val_accuracy: 0.3178\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 10s 341ms/step - loss: 0.5980 - accuracy: 0.4487 - val_loss: 0.7384 - val_accuracy: 0.2430\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 13s 423ms/step - loss: 0.5952 - accuracy: 0.4613 - val_loss: 0.7476 - val_accuracy: 0.2150\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 14s 463ms/step - loss: 0.5914 - accuracy: 0.4467 - val_loss: 0.7433 - val_accuracy: 0.2430\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 13s 450ms/step - loss: 0.5875 - accuracy: 0.4613 - val_loss: 0.7415 - val_accuracy: 0.2523\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 11s 367ms/step - loss: 0.5832 - accuracy: 0.4571 - val_loss: 0.7462 - val_accuracy: 0.2430\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 11s 350ms/step - loss: 0.5801 - accuracy: 0.4686 - val_loss: 0.7711 - val_accuracy: 0.1963\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 11s 357ms/step - loss: 0.5776 - accuracy: 0.4508 - val_loss: 0.7500 - val_accuracy: 0.2617\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 10s 338ms/step - loss: 0.5704 - accuracy: 0.4623 - val_loss: 0.7363 - val_accuracy: 0.2991\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 10s 334ms/step - loss: 0.5663 - accuracy: 0.4655 - val_loss: 0.7501 - val_accuracy: 0.2617\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 11s 381ms/step - loss: 0.5630 - accuracy: 0.4728 - val_loss: 0.7479 - val_accuracy: 0.2897\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 11s 366ms/step - loss: 0.5610 - accuracy: 0.4780 - val_loss: 0.7547 - val_accuracy: 0.2710\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 10s 331ms/step - loss: 0.5535 - accuracy: 0.4780 - val_loss: 0.7443 - val_accuracy: 0.3084\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 10s 326ms/step - loss: 0.5503 - accuracy: 0.4833 - val_loss: 0.7662 - val_accuracy: 0.2710\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 10s 325ms/step - loss: 0.5537 - accuracy: 0.4592 - val_loss: 0.7525 - val_accuracy: 0.3178\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 10s 329ms/step - loss: 0.5371 - accuracy: 0.4833 - val_loss: 0.7597 - val_accuracy: 0.2804\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 12s 413ms/step - loss: 0.5371 - accuracy: 0.4874 - val_loss: 0.7712 - val_accuracy: 0.2617\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 10s 336ms/step - loss: 0.5281 - accuracy: 0.4885 - val_loss: 0.7804 - val_accuracy: 0.2430\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.5246 - accuracy: 0.4780 - val_loss: 0.7738 - val_accuracy: 0.2897\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 10s 342ms/step - loss: 0.5194 - accuracy: 0.4948 - val_loss: 0.7759 - val_accuracy: 0.2710\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 10s 336ms/step - loss: 0.5098 - accuracy: 0.4958 - val_loss: 0.7807 - val_accuracy: 0.2897\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 10s 336ms/step - loss: 0.5062 - accuracy: 0.4958 - val_loss: 0.7724 - val_accuracy: 0.3364\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.5026 - accuracy: 0.4937 - val_loss: 0.7739 - val_accuracy: 0.3645\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 10s 349ms/step - loss: 0.4987 - accuracy: 0.5073 - val_loss: 0.7813 - val_accuracy: 0.3458\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.4902 - accuracy: 0.5115 - val_loss: 0.7756 - val_accuracy: 0.3458\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.4796 - accuracy: 0.5105 - val_loss: 0.7881 - val_accuracy: 0.3084\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 10s 342ms/step - loss: 0.4734 - accuracy: 0.5146 - val_loss: 0.7902 - val_accuracy: 0.3458\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.4629 - accuracy: 0.5209 - val_loss: 0.7968 - val_accuracy: 0.3084\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 0.4595 - accuracy: 0.5199 - val_loss: 0.8052 - val_accuracy: 0.3178\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 11s 353ms/step - loss: 0.4471 - accuracy: 0.5335 - val_loss: 0.8138 - val_accuracy: 0.3084\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 11s 352ms/step - loss: 0.4442 - accuracy: 0.5230 - val_loss: 0.8151 - val_accuracy: 0.3364\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 13s 440ms/step - loss: 0.4330 - accuracy: 0.5471 - val_loss: 0.8351 - val_accuracy: 0.2804\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 11s 378ms/step - loss: 0.4317 - accuracy: 0.5408 - val_loss: 0.8224 - val_accuracy: 0.3551\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.4220 - accuracy: 0.5408 - val_loss: 0.8359 - val_accuracy: 0.3178\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 11s 364ms/step - loss: 0.4058 - accuracy: 0.5596 - val_loss: 0.8800 - val_accuracy: 0.2617\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 10s 330ms/step - loss: 0.4032 - accuracy: 0.5523 - val_loss: 0.8523 - val_accuracy: 0.3178\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.3917 - accuracy: 0.5711 - val_loss: 0.9026 - val_accuracy: 0.2617\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 11s 375ms/step - loss: 0.3846 - accuracy: 0.5586 - val_loss: 0.8532 - val_accuracy: 0.3364\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.3764 - accuracy: 0.5711 - val_loss: 0.8685 - val_accuracy: 0.3178\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 11s 372ms/step - loss: 0.3732 - accuracy: 0.5732 - val_loss: 0.8753 - val_accuracy: 0.3178\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 11s 372ms/step - loss: 0.3571 - accuracy: 0.5879 - val_loss: 0.8872 - val_accuracy: 0.3364\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 11s 366ms/step - loss: 0.3476 - accuracy: 0.5805 - val_loss: 0.8737 - val_accuracy: 0.3364\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 0.3334 - accuracy: 0.5941 - val_loss: 0.8978 - val_accuracy: 0.3271\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 10s 348ms/step - loss: 0.3265 - accuracy: 0.6004 - val_loss: 0.9105 - val_accuracy: 0.3364\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 10s 347ms/step - loss: 0.3187 - accuracy: 0.5983 - val_loss: 0.9397 - val_accuracy: 0.2897\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 11s 351ms/step - loss: 0.3127 - accuracy: 0.6036 - val_loss: 0.9057 - val_accuracy: 0.3271\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 10s 343ms/step - loss: 0.3015 - accuracy: 0.6140 - val_loss: 0.9478 - val_accuracy: 0.3178\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 11s 354ms/step - loss: 0.2888 - accuracy: 0.6109 - val_loss: 0.9268 - val_accuracy: 0.3364\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 11s 355ms/step - loss: 0.2883 - accuracy: 0.6109 - val_loss: 0.9534 - val_accuracy: 0.3458\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 10s 345ms/step - loss: 0.2712 - accuracy: 0.6224 - val_loss: 0.9361 - val_accuracy: 0.3458\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 11s 363ms/step - loss: 0.2665 - accuracy: 0.6224 - val_loss: 0.9748 - val_accuracy: 0.3271\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.Input(shape=INPUT_DIMS)\n",
    "\n",
    "parallel_layers = []\n",
    "for size in REGION_SIZES:\n",
    "    parallel_layer = tf.keras.layers.Conv2D(FILTERS_PER_REGION, (INPUT_DIMS[0]), activation='relu')(input_layer)\n",
    "    parallel_layer = tf.keras.layers.MaxPool2D(pool_size=(1, size))(parallel_layer)\n",
    "    parallel_layers.append(parallel_layer)\n",
    "merged = tf.keras.layers.concatenate(parallel_layers, axis=2)\n",
    "flatten = tf.keras.layers.Flatten()(merged)\n",
    "hidden = tf.keras.layers.Dense(4, activation='relu')(flatten)\n",
    "output = tf.keras.layers.Dense(3, activation='softmax')(hidden)\n",
    "\n",
    "model = tf.keras.Model(input_layer, output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                   y_train,\n",
    "                   batch_size=32,\n",
    "                   epochs=100,\n",
    "                   validation_split=0.1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
